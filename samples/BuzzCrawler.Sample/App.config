<?xml version="1.0" encoding="utf-8" ?>
<configuration>
  <configSections>
    <section name="abot" type="Abot.Core.AbotConfigurationSectionHandler, Abot" />
  </configSections>
  <abot>
    <crawlBehavior maxConcurrentThreads="10" maxPagesToCrawl="100" maxPagesToCrawlPerDomain="0" maxPageSizeInBytes="0" userAgentString="Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko" crawlTimeoutSeconds="0" downloadableContentTypes="text/html, text/plain" isUriRecrawlingEnabled="false" isExternalPageCrawlingEnabled="false" isExternalPageLinksCrawlingEnabled="false" httpServicePointConnectionLimit="200" httpRequestTimeoutInSeconds="15" httpRequestMaxAutoRedirects="7" isHttpRequestAutoRedirectsEnabled="true" isHttpRequestAutomaticDecompressionEnabled="false" isSendingCookiesEnabled="false" isSslCertificateValidationEnabled="false" isRespectUrlNamedAnchorOrHashbangEnabled="false" minAvailableMemoryRequiredInMb="0" maxMemoryUsageInMb="0" maxMemoryUsageCacheTimeInSeconds="0" maxCrawlDepth="1000" isForcedLinkParsingEnabled="false" maxRetryCount="3" minRetryDelayInMilliseconds="1000" />
    <authorization isAlwaysLogin="false" loginUser="" loginPassword="" />
    <politeness isRespectRobotsDotTextEnabled="false" isRespectMetaRobotsNoFollowEnabled="false" isRespectHttpXRobotsTagHeaderNoFollowEnabled="false" isRespectAnchorRelNoFollowEnabled="false" isIgnoreRobotsDotTextIfRootDisallowedEnabled="false" robotsDotTextUserAgentString="abot" maxRobotsDotTextCrawlDelayInSeconds="5" minCrawlDelayPerDomainMilliSeconds="0" />
    <extensionValues>
      <add key="key1" value="value1" />
      <add key="key2" value="value2" />
    </extensionValues>
  </abot>
  <startup>
    <supportedRuntime version="v4.0" sku=".NETFramework,Version=v4.6.1" />
  </startup>
</configuration>